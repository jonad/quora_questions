{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal\n",
    "#### Djona Fegnem \n",
    "## Proposal: Identify question pairs with the same intent. \n",
    "### Domain Background<br />\n",
    "   \n",
    "   \n",
    "   Websites such as StackOverflow, Reddit, Quora provide a platform for users to ask and answer questions. The democratization of the Internet has enabled an increase engagement on such sites. StackOverflow since inception has roughly 13 million questions, 21 million answers, 56 million comments, and 7 million users as of April 1st, 2017. With this growing number of users and questions, some (websites) have started looking for ways to remove redundancies among questions. <br />\n",
    "   \n",
    "   One way to achieve such task is to limit the number of repeated questions. StackOverflow for instance, has tried to solve this problem by relying on its users. An attempt to post a question on StackOverflow provides the user with a search tool which allows him/her to find semantically related questions prior posting one with a set of guidelines on how to do so effectively. This first approach has some shortcomings: Firstly, the search functionality provided usually performs query search based on questions' word syntax - they tend to match questions with the same word form. Secondly, people use different vocabulary - users in different contexts, or with different needs, knowledge or linguistics habits will describe the same information using different terms. Finally, some users usually skip those guidelines and don't use the search tools.<br /> In addition to the search tool, StackOverflow has a duplicate questions policy, which allows its users to flag or vote to close duplicates. <br /> \n",
    "\n",
    "   Given these limitations in the existing tools and mechanisms, can it be a better alternative? <br />\n",
    "\n",
    "   With the growing number of data available on those sites, it becomes apparent that machine learning can be a solution. Machine learning enables computers to learn patterns from data thus, can learn patterns from data generated from users' interactions and provide a model that can automatically detect repeated questions.  This model can then be integrated into a search tool such that when a user attempts to ask a question, the search tool automatically searches for duplicates and provides an answer if it exists. This will improve the user experience and will allow for an efficient use of computer resources. <br />\n",
    "\n",
    "   In a recent Kaggle challenge, Quora has challenged the data science community to solve this task using their data. In [paper 2], the authors have used an artificial neural network  technique called convolution neural network (CNN) to solve similar problem. In this capstone project, we will apply the same technique to solve our problem. Our neural network will be trained using the Quora published question pairs dataset.\n",
    "\n",
    "### Problem Statement\n",
    "   Given a pair of sentences, can we build a model able to identify whether or not sentences within the pair are semantically equivalent? \n",
    "   By semantically equivalent we mean sentences or questions where the answers are the same.\n",
    "\n",
    "### Datasets and Inputs\n",
    "   The dataset used for this project is the Quora published question pairs dataset on Kaggle.\n",
    "Our dataset has 404,290 data points with the following features:\n",
    "    \n",
    "- Id: a unique identifier for each entry.\n",
    "- qid1: an identifier for the first question in a pair.\n",
    "- qid2: an identifier for the second question in a pair.\n",
    "- question1: the first question in a pair.\n",
    "- question2: the second question in a pair.\n",
    "- is_duplicate: a value 1 or 0 that indicates whether the questions in a pair are duplicate or not. \n",
    " \n",
    " \n",
    "### Solution Statement\n",
    "   We have a supervised learning task more specifically, a classification problem. Our goal is to build a classifier able to evaluate the similarity between two sentences. Since we have texts, we will need to perform feature engineering prior training.\n",
    "   During feature engineering, we will capture the meaning of sentences via low dimensional vectors also called word embedding. We will explore two word embedding techniques. One technique based on term co-occurence matrix (Latent Semantic Analysis - LSA)[paper 4], another technique based on neural network (word2vec)[paper 3]. This first step will give us feature vectors that will be used to train our model. During training we will evaluate two approaches:\n",
    "   In the first approach (baseline), we will use the embeddings from LSA to compute the cosine similarity score between sentences and use it to train our model. Our algorithm will be an ensemble learning algorithm called eXtreme Gradient Boosting (XGBoost). In the second approach we will train a deep neural network using pre-trained word2vec embeddings, the output will be compared against a threshold that we will choose during training. Our evaluation metric will be the f1 score.\n",
    "\n",
    "### Scoring method: Cosine similarity\n",
    "   For scoring method we will be using the cosine similarity. The cosine similarity measures the proximity of two vectors in the vector space. \n",
    "\n",
    "$$sim_q,_d = \\dfrac{\\sum_{i=1}^{|V|} q_i{d_i}}{\\sum_{i=1}^{|V|} q_i^{2} * \\sum_{i=1}^{|V|} d_i^{2}}$$\n",
    "\n",
    "\n",
    "\n",
    "where: \n",
    "\n",
    "   $sim_q,_d$ =  the cosine similarity of the document q and d.\n",
    "\n",
    "   $|V|$  = the size of the vocabulary.\n",
    "\n",
    "   $q_i$ = the tf-idf weight of term i in the document q.\n",
    "\n",
    "   $d_i$ = the tf-idf weight of term i in the document d.\n",
    "\n",
    "### Benchmark Model\n",
    "\n",
    "   Our baseline model will consist of the following steps: we will perform feature engineering on questions within a pair, and then we will apply XGBoost on the new dataset. Our feature engineering task will involve using an LSA model to extract questions' vector representation and computing the similarity score between questions of a pair.  Weights in the term-document matrix used to build the LSA model will be the Term Frequency-Inverse Document Frequency (TF-IDF )value between the document and the term. The similarity score, together with its corresponding class(duplicate 1, or no duplicate 0) value will form our new dataset.\n",
    "\n",
    "#### Term-document matrix with TF-IDF\n",
    "   A term-document matrix represents words or terms based on documents they contain. Values in the matrix can be either the word count, or the term frequency inverse document frequency(TF-IDF). The TF-IDF captures both the frequency of terms within the document and its rarity in the corpus. \n",
    "\n",
    "$$w_t,_d = (1 + {\\log_{10} tf_t,_d}) * \\log_{10}(\\dfrac{N}{df_t}) $$\n",
    "where: \n",
    "\n",
    "   $w_t,_d$ =  the tf-idf weigth of the term t in the document d.\n",
    "\n",
    "   $tf_t,d$  = the term frequency of t in in the document d.\n",
    "\n",
    "   $df_t$ = the document frequency of the term t.\n",
    "\n",
    "   $N$ = the total number of document in the corpus.\n",
    "   \n",
    "   We will use term-document matrix with TF-IDF weights during the modelling of our benchmark. Each document or question will consist of a TF-IDF vector in R^|V|, with |V| being our corpus' vocabulary size.<br />\n",
    "   TF stands for term frequency, it is the number of time a particular vocabulary appears in a document.<br />\n",
    "   IDF stands for inverse document frequency, it is the inverse of the number of documents in the corpus that contains a particular term.\n",
    "\n",
    "\n",
    "#### Baseline Feature engineering\n",
    "   Our baseline feature engineering will consist of the following: we will build our LSA model using questions in columns \"questions1\" and \"questions2\" as our set of documents. The weights in our term-document matrix will consist of the TF-IDF value between the term and the document. Next, we will extract the vector representation of each question. Finally, We will compute the similarity of each pair of questions using cosine similarity.\n",
    "\n",
    "<br />\n",
    "    ![Feature Engineering](feat_eng.png)\n",
    "    ![Training](training.png)\n",
    "<br>  \n",
    "\n",
    "### Evaluation Metric\n",
    "   For this project, we will use f1 score as evaluation metric. \n",
    "$$f1_{score} = 2* \\dfrac{precision*recall}{precision + recall }$$\n",
    "\n",
    "$$precision = \\dfrac{true_{positive}}{true_{positive} + false_{positive} }$$\n",
    "$$recall = \\dfrac{true_{positive}}{true_{positive} + false_{negative} }$$\n",
    "\n",
    "where: \n",
    "\n",
    "   $true_{positive}$ = the number of similar pairs of sentences our algorithm has successfully identified.\n",
    "\n",
    "   $false_{positive}$ = the number of similar pairs we have identified as non similar.\n",
    "\n",
    "   $false_{negative}$ = the number of non similar pairs we have identified as similar.\n",
    "   \n",
    "   The precision measures the proportion of positive instances correctly classified out of all the one returned by our model during evaluation.<br />\n",
    "   The recall measures the proportion of positive instances correctly classified out of all the positive instances returned by our model during our evaluation.<br />\n",
    "   f1 score is a single metric that captures both precision and recall.\n",
    "\n",
    "### Project Design\n",
    "   We will first perform an analysis of our data. It will involve removing any anomalies such as missing values, and non-alphanumeric characters. Then we will build and evaluate our benchmark model. After that, we will build and evaluate the model architecture described in the diagram below. It is a convolutional neural network (CNN) inspired from both [paper 5] and [paper 2]. The input is the tokenized text of questions within a pair, the output is the similarity score. The CNN first transforms words into real-valued feature vectors or word embeddings. Next, the convolutional layer is used to construct two distributed vector representations, one for each input question. Finally, the CNN computes the similarity score between them. Pairs of questions with a similarity above the threshold (hyperparameter that we will define during training) are considered duplicates.\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "   ![Convolutional Neural Network Archittecture](network2.png)\n",
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software requirements\n",
    "1. python 3.5.3.\n",
    "2. [gensim](https://radimrehurek.com/gensim/index.html) to build our word embedding. \n",
    "3. [tensorflow 1.1](https://keras.io/) : to build our neural network.\n",
    "\n",
    "### References\n",
    "1. [Quora dataset](https://www.kaggle.com/c/quora-question-pairs). \n",
    "2. [paper 1] - Convolutional Neural Networks for Sentence Classification, Yoon Kim, New York University.<br />\n",
    "3. [paper 2] - Detecting Semantically Equivalent Questions in Online User Forums, Dasha Bogdanova∗, C ́ıcero dos Santos†, Luciano Barbosa† and Bianca Zadrozny† ∗ADAPT centre, School of Computing, Dublin City University, Dublin, Ireland <br />\n",
    "4. [paper 3] - Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, Google Inc., Mountain View, CA <br />\n",
    "5. [paper 4] - Indexing by Latent Semantic Analysis, Scott Deerwester, sept 1990. <br />\n",
    "6. [paper 5] - A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification, Ye Zhang, Byron C. Wallace. <br />\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
